{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from thop import profile  # FLOPs 계산용\n",
    "\n",
    "# 기존 모델들 import\n",
    "from model import CircleNet, CircleDataset\n",
    "from pruning import CircleNet_P\n",
    "from quantization import CircleNet_Q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # Basic 모델\n",
    "    basic_model = CircleNet()\n",
    "    basic_model.load_state_dict(torch.load('Basic_model.pth'))\n",
    "    \n",
    "    # Pruned 모델\n",
    "    pruned_model = CircleNet_P(pruning_ratio=0.5)\n",
    "    pruned_model.load_state_dict(torch.load('P_model.pth'))\n",
    "    \n",
    "    # Quantized 모델\n",
    "    quantized_model = CircleNet_Q()\n",
    "    quantized_model.load_state_dict(torch.load('Q_model.pth'))\n",
    "    \n",
    "    return basic_model, pruned_model, quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flops(model, input_size=(1, 1, 416, 416)):\n",
    "    \"\"\"모델의 FLOPs 계산\"\"\"\n",
    "    input = torch.randn(input_size)\n",
    "    flops, params = profile(model, inputs=(input, ))\n",
    "    return flops, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_metrics(model, test_loader, device, model_name):\n",
    "    \"\"\"모델의 모든 메트릭 측정\"\"\"\n",
    "    print(f\"\\nMeasuring metrics for {model_name}...\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 1. 모델 크기 및 파라미터\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    torch.save(model.state_dict(), 'temp.pth')\n",
    "    model_size = os.path.getsize('temp.pth') / (1024 * 1024)  # MB\n",
    "    os.remove('temp.pth')\n",
    "    \n",
    "    # 2. FLOPs 계산\n",
    "    try:\n",
    "        flops, _ = calculate_flops(model)\n",
    "    except Exception as e:\n",
    "        print(f\"FLOPs calculation failed: {e}\")\n",
    "        flops = 0\n",
    "    \n",
    "    # 3. 추론 성능 측정\n",
    "    model.eval()\n",
    "    inference_times = []\n",
    "    memory_usage = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # 메모리 사용량\n",
    "            memory_usage.append(psutil.Process().memory_info().rss / 1024**2)\n",
    "            \n",
    "            # 추론 시간\n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            inference_times.append((time.time() - start_time) * 1000)\n",
    "            \n",
    "            # 정확도\n",
    "            pred = outputs.round()\n",
    "            acc = (torch.abs(pred - targets) <= 1).float().mean().item()\n",
    "            accuracy.append(acc)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Parameters': param_count,\n",
    "        'Model_Size_MB': model_size,\n",
    "        'GFLOPs': flops / 1e9,\n",
    "        'Avg_Inference_Time_ms': np.mean(inference_times),\n",
    "        'Inference_Time_Std_ms': np.std(inference_times),\n",
    "        'Memory_Usage_MB': np.mean(memory_usage),\n",
    "        'Accuracy_%': np.mean(accuracy) * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_size(model):\n",
    "    \"\"\"모델 크기 측정 (파라미터 수, 메모리 사용량)\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return {\n",
    "        'parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'size_mb': size_mb\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hardware_usage(model, test_loader):\n",
    "    \"\"\"Jetson Nano 하드웨어 사용량 측정\"\"\"\n",
    "    with jtop() as jetson:\n",
    "        stats = {\n",
    "            'cpu_usage': [],\n",
    "            'gpu_usage': [],\n",
    "            'memory_usage': [],\n",
    "            'power_usage': []\n",
    "        }\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in test_loader:\n",
    "                _ = model(images)\n",
    "                \n",
    "                stats['cpu_usage'].append(jetson.cpu['cpu']['usage'])\n",
    "                stats['gpu_usage'].append(jetson.gpu['gpu']['usage'])\n",
    "                stats['memory_usage'].append(jetson.memory['used'] / jetson.memory['total'] * 100)\n",
    "                stats['power_usage'].append(jetson.power['power'])\n",
    "    \n",
    "    return {k: np.mean(v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results_df):\n",
    "    \"\"\"결과 시각화\"\"\"\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. 모델 크기 vs 정확도\n",
    "    plt.subplot(231)\n",
    "    plt.scatter(results_df['Model_Size_MB'], results_df['Accuracy_%'], s=100)\n",
    "    for i, model in enumerate(results_df['Model']):\n",
    "        plt.annotate(model, (results_df['Model_Size_MB'][i], results_df['Accuracy_%'][i]))\n",
    "    plt.xlabel('Model Size (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Size vs Accuracy')\n",
    "    \n",
    "    # 2. 추론 시간 비교\n",
    "    plt.subplot(232)\n",
    "    bar1 = plt.bar(results_df['Model'], results_df['Avg_Inference_Time_ms'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Inference Time (ms)')\n",
    "    plt.title('Average Inference Time')\n",
    "    \n",
    "    # 3. 메모리 사용량 비교\n",
    "    plt.subplot(233)\n",
    "    bar2 = plt.bar(results_df['Model'], results_df['Memory_Usage_MB'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Usage')\n",
    "    \n",
    "    # 4. GFLOPs 비교\n",
    "    plt.subplot(234)\n",
    "    bar3 = plt.bar(results_df['Model'], results_df['GFLOPs'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('GFLOPs')\n",
    "    plt.title('Computational Complexity')\n",
    "    \n",
    "    # 5. 파라미터 수 비교\n",
    "    plt.subplot(235)\n",
    "    bar4 = plt.bar(results_df['Model'], results_df['Parameters'] / 1e6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Parameters (M)')\n",
    "    plt.title('Number of Parameters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_results.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    basic_model, pruned_model, quantized_model = load_models()\n",
    "    \n",
    "    # 테스트 데이터셋 준비\n",
    "    test_dataset = CircleDataset('test/img', 'test/target')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # 각 모델 평가\n",
    "    results = []\n",
    "    results.append(measure_model_metrics(basic_model, test_loader, device, \"Basic\"))\n",
    "    results.append(measure_model_metrics(pruned_model, test_loader, device, \"Pruned\"))\n",
    "    results.append(measure_model_metrics(quantized_model, test_loader, device, \"Quantized\"))\n",
    "    \n",
    "    # 결과를 DataFrame으로 변환\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plot_metrics(results_df)\n",
    "    \n",
    "    # 결과 저장\n",
    "    results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight: torch.Size([16, 1, 3, 3])\n",
      "features.0.bias: torch.Size([16])\n",
      "features.3.weight: torch.Size([32, 16, 3, 3])\n",
      "features.3.bias: torch.Size([32])\n",
      "features.6.weight: torch.Size([64, 32, 3, 3])\n",
      "features.6.bias: torch.Size([64])\n",
      "classifier.1.weight: torch.Size([128, 3136])\n",
      "classifier.1.bias: torch.Size([128])\n",
      "classifier.4.weight: torch.Size([1, 128])\n",
      "classifier.4.bias: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghddp\\AppData\\Local\\Temp\\ipykernel_9300\\620319156.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('Basic_model.pth')\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('Basic_model.pth')\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghddp\\AppData\\Local\\Temp\\ipykernel_9300\\3231421482.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  basic_model.load_state_dict(torch.load('Basic_model.pth'))\n",
      "C:\\Users\\ghddp\\AppData\\Local\\Temp\\ipykernel_9300\\3231421482.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pruned_model.load_state_dict(torch.load('P_model.pth'))\n",
      "C:\\Users\\ghddp\\AppData\\Local\\Temp\\ipykernel_9300\\3231421482.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quantized_model.load_state_dict(torch.load('Q_model.pth'))\n",
      "c:\\Users\\ghddp\\anaconda3\\lib\\site-packages\\torch\\_utils.py:404: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CircleNet_Q:\n\tMissing key(s) in state_dict: \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.scale\", \"features.0.zero_point\", \"features.3.scale\", \"features.3.zero_point\", \"features.6.scale\", \"features.6.zero_point\", \"classifier.1.scale\", \"classifier.1.zero_point\", \"classifier.1._packed_params.dtype\", \"classifier.1._packed_params._packed_params\", \"classifier.4.scale\", \"classifier.4.zero_point\", \"classifier.4._packed_params.dtype\", \"classifier.4._packed_params._packed_params\", \"quant.scale\", \"quant.zero_point\". \n\tWhile copying the parameter named \"features.0.weight\", whose dimensions in the model are torch.Size([16, 1, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 1, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"features.3.weight\", whose dimensions in the model are torch.Size([32, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 16, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"features.6.weight\", whose dimensions in the model are torch.Size([64, 32, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 32, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m basic_model, pruned_model, quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 테스트 데이터셋 준비\u001b[39;00m\n\u001b[0;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CircleDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/target\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mload_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Quantized 모델\u001b[39;00m\n\u001b[0;32m     11\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m CircleNet_Q()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mquantized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQ_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m basic_model, pruned_model, quantized_model\n",
      "File \u001b[1;32mc:\\Users\\ghddp\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CircleNet_Q:\n\tMissing key(s) in state_dict: \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.scale\", \"features.0.zero_point\", \"features.3.scale\", \"features.3.zero_point\", \"features.6.scale\", \"features.6.zero_point\", \"classifier.1.scale\", \"classifier.1.zero_point\", \"classifier.1._packed_params.dtype\", \"classifier.1._packed_params._packed_params\", \"classifier.4.scale\", \"classifier.4.zero_point\", \"classifier.4._packed_params.dtype\", \"classifier.4._packed_params._packed_params\", \"quant.scale\", \"quant.zero_point\". \n\tWhile copying the parameter named \"features.0.weight\", whose dimensions in the model are torch.Size([16, 1, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 1, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"features.3.weight\", whose dimensions in the model are torch.Size([32, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 16, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"features.6.weight\", whose dimensions in the model are torch.Size([64, 32, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 32, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',)."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
