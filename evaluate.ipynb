{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import cv2\n",
    "\n",
    "from model import CircleNet as CircleNet, CircleDataset\n",
    "from pruning import CircleNet_P as CircleNet_P\n",
    "from quantization import CircleNet_Q as CircleNet_Q\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "# import jtop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # 원본 모델 로드\n",
    "    original_model = CircleNet()\n",
    "    original_model.load_state_dict(torch.load('model/original_model.pth'))\n",
    "    \n",
    "    # Pruned 모델들 로드 (여러 ratio)\n",
    "    pruned_models = {}\n",
    "    for ratio in [0.3, 0.5, 0.7]:\n",
    "        model = CircleNet_P(original_model, pruning_ratio=ratio)\n",
    "        model.load_state_dict(torch.load(f'models/pruned_model_{int(ratio*100)}.pth'))\n",
    "        pruned_models[f'pruned_{int(ratio*100)}'] = model\n",
    "    \n",
    "    # Quantized 모델들 로드 (8-bit, 4-bit)\n",
    "    quantized_models = {}\n",
    "    for bits in [8, 4]:\n",
    "        model = CircleNet_Q()\n",
    "        model.load_state_dict(torch.load(f'models/quantized_model_{bits}bit.pth'))\n",
    "        quantized_models[f'quantized_{bits}bit'] = model\n",
    "    \n",
    "    return original_model, pruned_models, quantized_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_size(model):\n",
    "    \"\"\"모델 크기 측정 (파라미터 수, 메모리 사용량)\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return {\n",
    "        'parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'size_mb': size_mb\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_performance(model, test_loader, num_runs=100):\n",
    "    \"\"\"추론 성능 측정 (속도, 정확도, 메모리)\"\"\"\n",
    "    model.eval()\n",
    "    inference_times = []\n",
    "    memory_usage = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            for images, targets in test_loader:\n",
    "                # 메모리 사용량 측정\n",
    "                memory_usage.append(psutil.Process().memory_info().rss / 1024**2)\n",
    "                \n",
    "                # 추론 시간 측정\n",
    "                start_time = time.time()\n",
    "                outputs = model(images)\n",
    "                inference_times.append((time.time() - start_time) * 1000)  # ms 단위\n",
    "                \n",
    "                # 정확도 계산 (예측값과 실제값의 차이가 1 이하인 경우 정확)\n",
    "                pred = outputs.round()\n",
    "                acc = (torch.abs(pred - targets) <= 1).float().mean().item()\n",
    "                accuracy.append(acc)\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': np.mean(inference_times),\n",
    "        'std_inference_time': np.std(inference_times),\n",
    "        'avg_memory_usage': np.mean(memory_usage),\n",
    "        'accuracy': np.mean(accuracy) * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hardware_usage(model, test_loader):\n",
    "    \"\"\"Jetson Nano 하드웨어 사용량 측정\"\"\"\n",
    "    with jtop() as jetson:\n",
    "        stats = {\n",
    "            'cpu_usage': [],\n",
    "            'gpu_usage': [],\n",
    "            'memory_usage': [],\n",
    "            'power_usage': []\n",
    "        }\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in test_loader:\n",
    "                _ = model(images)\n",
    "                \n",
    "                stats['cpu_usage'].append(jetson.cpu['cpu']['usage'])\n",
    "                stats['gpu_usage'].append(jetson.gpu['gpu']['usage'])\n",
    "                stats['memory_usage'].append(jetson.memory['used'] / jetson.memory['total'] * 100)\n",
    "                stats['power_usage'].append(jetson.power['power'])\n",
    "    \n",
    "    return {k: np.mean(v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(results):\n",
    "    \"\"\"성능 비교 시각화\"\"\"\n",
    "    # 결과를 DataFrame으로 변환\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 1. 메모리 vs 정확도\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['size_mb'], df['accuracy'])\n",
    "    for i, model in enumerate(df.index):\n",
    "        plt.annotate(model, (df['size_mb'][i], df['accuracy'][i]))\n",
    "    plt.xlabel('Model Size (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Size vs Accuracy Trade-off')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. 추론 시간 비교\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df.index, df['avg_inference_time'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Inference Time (ms)')\n",
    "    plt.title('Average Inference Time Comparison')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. 하드웨어 사용량 히트맵\n",
    "    hardware_metrics = ['cpu_usage', 'gpu_usage', 'memory_usage', 'power_usage']\n",
    "    hw_data = df[hardware_metrics]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(hw_data.T, annot=True, fmt='.1f', cmap='YlOrRd')\n",
    "    plt.title('Hardware Resource Usage Comparison')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 모델 로드\n",
    "    original_model, pruned_models, quantized_models = load_models()\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_dataset = CircleDataset('test/img', 'test/target')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # 결과 저장할 딕셔너리\n",
    "    results = {}\n",
    "    \n",
    "    # 1. 원본 모델 평가\n",
    "    print(\"Evaluating original model...\")\n",
    "    results['original'] = {\n",
    "        **measure_model_size(original_model),\n",
    "        **measure_inference_performance(original_model, test_loader),\n",
    "        **measure_hardware_usage(original_model, test_loader)\n",
    "    }\n",
    "    \n",
    "    # 2. Pruned 모델들 평가\n",
    "    for name, model in pruned_models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        results[name] = {\n",
    "            **measure_model_size(model),\n",
    "            **measure_inference_performance(model, test_loader),\n",
    "            **measure_hardware_usage(model, test_loader)\n",
    "        }\n",
    "    \n",
    "    # 3. Quantized 모델들 평가\n",
    "    for name, model in quantized_models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        results[name] = {\n",
    "            **measure_model_size(model),\n",
    "            **measure_inference_performance(model, test_loader),\n",
    "            **measure_hardware_usage(model, test_loader)\n",
    "        }\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plot_performance_comparison(results)\n",
    "    \n",
    "    # 결과를 CSV로 저장\n",
    "    pd.DataFrame(results).T.to_csv('model_comparison_results.csv')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghddp\\AppData\\Local\\Temp\\ipykernel_28700\\2566148516.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load('CircleDetection\\model\\CircleNet.pth'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CircleDetection\\\\model\\\\CircleNet.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary of results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     original_model, pruned_models, quantized_models \u001b[38;5;241m=\u001b[39m \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 테스트 데이터 로드\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m CircleDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/target\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mload_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_models\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 원본 모델 로드\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     original_model \u001b[38;5;241m=\u001b[39m CircleNet()\n\u001b[1;32m----> 4\u001b[0m     original_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCircleDetection\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCircleNet.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Pruned 모델들 로드 (여러 ratio)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     pruned_models \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\ghddp\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\ghddp\\anaconda3\\lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\ghddp\\anaconda3\\lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CircleDetection\\\\model\\\\CircleNet.pth'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    print(\"\\nSummary of results:\")\n",
    "    print(pd.DataFrame(results).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
